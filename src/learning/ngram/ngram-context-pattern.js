/**
 * NgramContextPatternAI - N-gramè¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜AI
 * 
 * Variable-order N-gramã¨Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’ç”¨ã„ã¦ã€
 * ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€äºˆæ¸¬ã—ã¾ã™ã€‚
 */
import { persistentLearningDB } from '../../data/persistent-learning-db.js';

/**
 * NgramContextPatternAI - N-gramè¨€èªãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜AI
 * 
 * Variable-order N-gramã¨Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’ç”¨ã„ã¦ã€
 * ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã€äºˆæ¸¬ã—ã¾ã™ã€‚
 */
export class NgramContextPatternAI {
  constructor(maxNgramOrder = 3) { // Add maxNgramOrder to constructor
    this.ngramFrequencies = new Map(); // Map<ngram: string, frequency: number>
    this.contextFrequencies = new Map(); // Map<context: string, frequency: number>
    this.totalNgrams = 0;
    this.maxNgramOrder = maxNgramOrder; // Store maxNgramOrder
    this.isInitialized = false;
  }

  async initialize() {
    if (this.isInitialized) return;
    console.log('ğŸ§¬ NgramContextPatternAIåˆæœŸåŒ–ä¸­...');
    try {
      const loadedData = await persistentLearningDB.loadNgramData();
      if (loadedData) {
        this.ngramFrequencies = new Map(loadedData.ngramFrequencies);
        this.contextFrequencies = new Map(loadedData.contextFrequencies);
        this.totalNgrams = loadedData.totalNgrams;
        console.log(`âœ… NgramContextPatternAIåˆæœŸåŒ–å®Œäº†ã€‚${this.ngramFrequencies.size}ä»¶ã®N-gramçµ±è¨ˆã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸã€‚`);
      } else {
        console.log('âœ… NgramContextPatternAIåˆæœŸåŒ–å®Œäº†ã€‚æ–°è¦ãƒ‡ãƒ¼ã‚¿ã€‚');
      }
    } catch (error) {
      console.error('âŒ NgramContextPatternAIåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼:', error.message);
    }
    this.isInitialized = true;
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰N-gramãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ç¿’ã—ã¾ã™ã€‚
   * @param {string} text - å­¦ç¿’å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
   * @param {object} contextInfo - ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆæƒ…å ± (ä¾‹: { category: 'technical' })
   */
  async learnPattern(text, contextInfo) {
    if (!this.isInitialized) {
      await this.initialize();
    }

    // ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã“ã“ã§ã¯ç°¡æ˜“çš„ã«ã‚¹ãƒšãƒ¼ã‚¹ã§åˆ†å‰²ï¼‰
    const tokens = text.split(/\s+/).filter(token => token.length > 0);

    // N-gramã®ç”Ÿæˆã¨é »åº¦å­¦ç¿’
    for (let n = 1; n <= this.maxNgramOrder; n++) { // Loop for different N-gram orders
      for (let i = 0; i <= tokens.length - n; i++) {
        const ngram = tokens.slice(i, i + n).join(' ');
        this.updateNgramFrequency(ngram);
      }
    }

    // æ–‡è„ˆé »åº¦ã®å­¦ç¿’
    if (contextInfo && contextInfo.category) {
      this.updateContextFrequency(contextInfo.category);
    }
    await this._saveData();
  }

  /**
   * N-gramã®é »åº¦ã‚’æ›´æ–°ã—ã¾ã™ã€‚
   * @param {string} ngram - æ›´æ–°ã™ã‚‹N-gram
   */
  updateNgramFrequency(ngram) {
    this.ngramFrequencies.set(ngram, (this.ngramFrequencies.get(ngram) || 0) + 1);
    this.totalNgrams++; // This should probably count unique ngrams or total tokens, but for now, keep as is.
  }

  /**
   * æ–‡è„ˆã®é »åº¦ã‚’æ›´æ–°ã—ã¾ã™ã€‚
   * @param {string} context - æ›´æ–°ã™ã‚‹æ–‡è„ˆ
   */
  updateContextFrequency(context) {
    this.contextFrequencies.set(context, (this.contextFrequencies.get(context) || 0) + 1);
  }

  /**
   * ãƒ†ã‚­ã‚¹ãƒˆã®æ–‡è„ˆã‚’äºˆæ¸¬ã—ã¾ã™ã€‚
   * (ç°¡æ˜“çš„ãªå®Ÿè£…ã€‚å®Ÿéš›ã«ã¯Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ãªã©ã‚’ç”¨ã„ã‚‹)
   * @param {string} text - äºˆæ¸¬å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆ
   * @returns {object} äºˆæ¸¬ã•ã‚ŒãŸæ–‡è„ˆæƒ…å ±
   */
  async predictContext(text) {
    if (!this.isInitialized) {
      await this.initialize();
    }

    const tokens = text.split(/\s+/).filter(token => token.length > 0);
    let bestContext = null;
    let maxScore = -Infinity;

    // Iterate through all learned contexts
    for (const [context, contextFreq] of this.contextFrequencies.entries()) {
      let currentContextScore = 0;
      // For each context, calculate a score based on matching n-grams in the text
      for (let n = 1; n <= this.maxNgramOrder; n++) {
        for (let i = 0; i <= tokens.length - n; i++) {
          const ngram = tokens.slice(i, i + n).join(' ');
          const ngramFreq = this.ngramFrequencies.get(ngram) || 0;
          // A very simplified scoring: sum of ngram frequencies, weighted by context frequency
          // In a real scenario, this would involve conditional probabilities and Kneser-Ney
          currentContextScore += ngramFreq * (contextFreq / this.totalNgrams); // Simplified weighting
        }
      }

      if (currentContextScore > maxScore) {
        maxScore = currentContextScore;
        bestContext = context;
      }
    }
    
    // If no context is found, fallback to the most frequent context or a default
    if (bestContext === null && this.contextFrequencies.size > 0) {
        let mostFrequentContext = null;
        let highestFreq = 0;
        for (const [context, freq] of this.contextFrequencies.entries()) {
            if (freq > highestFreq) {
                highestFreq = freq;
                mostFrequentContext = context;
            }
        }
        bestContext = mostFrequentContext;
        // Confidence calculation needs to be more robust
        return { predictedCategory: bestContext, confidence: highestFreq / this.totalNgrams };
    } else if (bestContext === null) {
        return { predictedCategory: 'general', confidence: 0 }; // Default if no learning has occurred
    }

    // Confidence calculation is still very basic and needs refinement for real Kneser-Ney
    return { predictedCategory: bestContext, confidence: maxScore > 0 ? maxScore / this.totalNgrams : 0 };
  }

  /**
   * Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã‚’ç”¨ã„ãŸN-gramã®ç¢ºç‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚
   * (ã“ã‚Œã¯ã‚¹ã‚±ãƒ«ãƒˆãƒ³ã§ã‚ã‚Šã€å®Ÿéš›ã®Kneser-Neyå®Ÿè£…ã¯ã‚ˆã‚Šè¤‡é›‘ã§ã™)
   * @param {string} ngram - è¨ˆç®—å¯¾è±¡ã®N-gram
   * @param {number} order - N-gramã®æ¬¡æ•° (ä¾‹: 2 for bigram)
   * @returns {number} ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã•ã‚ŒãŸç¢ºç‡
   */
  calculateSmoothProbability(ngram, order) {
    // Kneser-Neyã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°ã®åŸºæœ¬æ¦‚å¿µ:
    // P_KN(w_i | w_{i-n+1}...w_{i-1}) = max(count(w_{i-1}w_i) - d, 0) / count(w_{i-1}) + lambda(w_{i-1}) * P_KN(w_i | w_{i-n+2}...w_{i-1})
    // ã“ã“ã§ã¯ã€ãã®æ¦‚å¿µã‚’åæ˜ ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ãƒ¼ã‚’æä¾›ã—ã¾ã™ã€‚

    const frequency = this.ngramFrequencies.get(ngram) || 0;
    if (order === 1) {
      // Unigram probability (smoothed)
      return frequency / this.totalNgrams; // Simplified
    } else {
      // Higher-order N-gram probability (simplified Kneser-Ney idea)
      // This would involve:
      // 1. Discounting (d) for observed n-grams
      // 2. Lower-order probability (recursive call or pre-calculated)
      // 3. Lambda term (context-dependent smoothing parameter)

      // For now, a very basic smoothed frequency
      const prefix = ngram.split(' ').slice(0, order - 1).join(' ');
      const prefixFreq = this.ngramFrequencies.get(prefix) || 1; // Avoid division by zero
      
      // This is NOT Kneser-Ney, just a slightly less naive probability
      return (frequency + 0.1) / (prefixFreq + 0.1 * this.ngramFrequencies.size); // Add-alpha smoothing idea
    }
  }

  /**
   * N-gramãƒ‡ãƒ¼ã‚¿ã‚’æ°¸ç¶šåŒ–ã—ã¾ã™ã€‚
   */
  async _saveData() {
    const dataToSave = {
      ngramFrequencies: Array.from(this.ngramFrequencies.entries()),
      contextFrequencies: Array.from(this.contextFrequencies.entries()),
      totalNgrams: this.totalNgrams,
    };
    await persistentLearningDB.saveNgramData(dataToSave);
    console.log('ğŸ’¾ N-gramãƒ‡ãƒ¼ã‚¿ä¿å­˜å®Œäº†');
  }
}
